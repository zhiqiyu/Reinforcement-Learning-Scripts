{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jZMDpmLPPXy_"
   },
   "source": [
    "# Solve Lunar Lander with DQN\n",
    "\n",
    "the LL environ- ment is a simulated environment where the agent needs to successfully and safely land the aircraft in the designated area. The lunar lander has 4 discrete actions, do nothing, fire the left orientation engine, fire the main engine, and fire the right orientation engine. The states of the lander are represented as 8-dimensional vectors:\n",
    "`(x, y, vx, vy, θ, vθ, lef tleg, rightleg)`, \n",
    "x and y are the x and y-coordinates of the lunar lander’s position on the screen. vx and vy are the lunar lander’s velocity components on the x and y axes.θ is the angle of the lunar lander. vθ is the angular velocity of the lander. Finally, leftleg and rightleg are binary values to indicate whether the left leg or right leg of the lunar lander is touching the ground. The lunar lander starts as (0,0) and the target landing pad is at (0, 1). The total reward for moving from starting point to landing pad ranges from 100 - 140 points varying on lander placement on the pad. If lander moves away from landing pad it is penalized the amount of reward that would be gained by moving towards the pad. An episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points respectively. Each leg ground contact is rewarded with +10 points. Each firing main engine action has a -0.3 point reward, but fuels are infinite. Lastly, the LL problem is considered solved when an average of 200 points is achieved over 100 consecutive runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4879,
     "status": "ok",
     "timestamp": 1572309057672,
     "user": {
      "displayName": "Zhiqi Yu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBi-8Tmka31UxkN8y4ziOk_1QXx0krmLXgBn1kSng=s64",
      "userId": "01942841542704611946"
     },
     "user_tz": 240
    },
    "id": "h9CiCCwtB2dg",
    "outputId": "709acdf1-18c8-4d3e-81d6-d6c859a07a6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting box2d-py\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/bd/6cdc3fd994b0649dcf5d9bad85bd9e26172308bbe9a421bfc6fdbf5081a6/box2d_py-2.3.8-cp36-cp36m-manylinux1_x86_64.whl (448kB)\n",
      "\u001b[K     |████████████████████████████████| 450kB 3.5MB/s \n",
      "\u001b[?25hInstalling collected packages: box2d-py\n",
      "Successfully installed box2d-py-2.3.8\n"
     ]
    }
   ],
   "source": [
    "!pip install box2d-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FWUWn9uRPXzB"
   },
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "import time\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "_ = torch.manual_seed(1027)\n",
    "np.random.seed(1027)\n",
    "random.seed(1027)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4fEPzC7TPXzG"
   },
   "source": [
    "## Experience Replay\n",
    "\n",
    "This is the key to train DQN, it serves to remove strong correlations between consecutive transitions because transitions are randomly sampled for training with experience replay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_rkkIQsJPXzH"
   },
   "outputs": [],
   "source": [
    "Experience = namedtuple(\"Experience\", ('state', 'action', 'reward', 'next_state', 'is_done'))\n",
    "\n",
    "class ExpReplay:\n",
    "    def __init__(self, capacity=100000, starting_size=20000):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "        self.starting_size = starting_size\n",
    "        \n",
    "    def add(self, transition):\n",
    "            self.memory.append(transition)\n",
    "            \n",
    "    def sample(self, batch_size=16):\n",
    "        if batch_size > len(self.memory):\n",
    "            return None\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def canStart(self):\n",
    "        return len(self.memory) >= self.starting_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HebkG8Z3PXzJ"
   },
   "source": [
    "## Function Approximation\n",
    "\n",
    "An artificial neural network (ANN) is used as function approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sU2AKo9zPXzK"
   },
   "outputs": [],
   "source": [
    "class QNN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dims=[20, 20]):\n",
    "        super(QNN, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.n_action = action_dim\n",
    "        \n",
    "        n_nodes = [state_dim] + hidden_dims + [action_dim]\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for i in range(1, len(n_nodes)):\n",
    "            self.layers.append(nn.Linear(n_nodes[i-1], n_nodes[i]))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \n",
    "        # feed through all layers\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            X = torch.tanh(self.layers[i](X))\n",
    "        \n",
    "        out = self.layers[-1](X)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qHaLhEh7PXzM"
   },
   "outputs": [],
   "source": [
    "# loss function\n",
    "def loss_fn(yPred, yTrue):\n",
    "    return torch.nn.SmoothL1Loss()(yPred, yTrue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rQX1iqoyaIT4"
   },
   "source": [
    "## The Lunar Lander Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qurx96XFPXzO"
   },
   "outputs": [],
   "source": [
    "class LunarLanderAgent:\n",
    "    def __init__(self, env, policy_net, target_net, exp_replay):\n",
    "        self.env = env\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.n_action = env.action_space.n\n",
    "        \n",
    "        # intialize NN for Q function approximation, and associated target net\n",
    "        self.pn = policy_net.to(device)               \n",
    "        self.tn = target_net.to(device)              \n",
    "        self.tn.load_state_dict(self.pn.state_dict())\n",
    "        self.tn.eval()\n",
    "        \n",
    "        # initialize experience replay\n",
    "        self.memo = exp_replay   \n",
    "    \n",
    "    def __wrapState(self, state):\n",
    "        \"\"\"Wrap state tuples into a torch.tensor\"\"\"\n",
    "        return torch.tensor(state, dtype=torch.float32, device=device).reshape(1, -1)\n",
    "    \n",
    "    def chooseAction(self, state, epsilon):\n",
    "        \"\"\"\n",
    "        Choose action using the epsilon greedy policy, with epsilon probability to choose a random action, \n",
    "        otherwise stick with policy.\n",
    "        \"\"\"\n",
    "        explore = (torch.rand(1) < epsilon)\n",
    "        if explore:  \n",
    "            # need to explore\n",
    "            return torch.randint(0, self.n_action, (1,)).item()\n",
    "        else:        \n",
    "            # pick the best move\n",
    "            state = self.__wrapState(state)\n",
    "            with torch.no_grad():\n",
    "                action = self.pn(state).detach().argmax().item()\n",
    "            return action\n",
    "        \n",
    "    def play(self, render=False, sleep_time=0.1):\n",
    "        '''\n",
    "        Run the agent in the environment once, return the total reward if no rendering\n",
    "        is needed, otherwise, render the environment with given time intervals between \n",
    "        each frame.\n",
    "        '''\n",
    "        total_reward = 0\n",
    "        state = self.env.reset()\n",
    "        while True:\n",
    "            with torch.no_grad():\n",
    "                action = self.pn(self.__wrapState(state)).argmax().item()\n",
    "            new_state, reward, done, _ = self.env.step(action)\n",
    "            if render:\n",
    "                time.sleep(sleep_time)\n",
    "                self.env.render()\n",
    "            total_reward += reward\n",
    "            state = new_state\n",
    "            if done:\n",
    "                break\n",
    "        if render:\n",
    "            print(\"Total reward: %d\" % total_reward)\n",
    "            self.env.close()\n",
    "        else:\n",
    "            return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g1i_OUTYPXzQ"
   },
   "source": [
    "## Training\n",
    "\n",
    "The training procedure follows the original algorithm described in Mnih et al. (2015) like below:\n",
    "\n",
    "![DQN algorithm]('dqnalg.png')\n",
    "\n",
    "Something different is that an early-stopping flag is added to early stop training when the agent is already able to confidently (90%) land on the landing pad with > 200 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t15qB8RO3kER"
   },
   "outputs": [],
   "source": [
    "def processBatch(batch):\n",
    "    '''Wrap elements of mini-batches into torch tensors'''\n",
    "    state_dim = batch[0].state.shape[0]\n",
    "\n",
    "    X = torch.empty((len(batch), state_dim), dtype=torch.float32)\n",
    "    X_next = torch.empty((len(batch), state_dim), dtype=torch.float32)\n",
    "    rewards = torch.empty((len(batch), 1), dtype=torch.float32)\n",
    "    actions = torch.zeros((len(batch,)), dtype=torch.long)\n",
    "    not_dones = torch.tensor([True] * len(batch), dtype=torch.bool)\n",
    "    for i, transition in enumerate(batch):\n",
    "        X[i, :] = torch.tensor(transition.state, dtype=torch.float32)            # current state\n",
    "        X_next[i, :] = torch.tensor(transition.next_state, dtype=torch.float32)  # next state\n",
    "        actions[i] = transition.action                                           # action\n",
    "        rewards[i, :] = transition.reward                                        # rewards\n",
    "        not_dones[i] = not transition.is_done                                    # is terminal state\n",
    "        \n",
    "    return X.to(device), X_next.to(device), actions.to(device), rewards.to(device), not_dones.to(device)\n",
    "\n",
    "\n",
    "def train(agent, optimizer, episodes=500, batch_size=32, update_rate=1, gamma=0.99, \n",
    "            max_epsilon=1, min_epsilon=0.1, eps_decay=200, reward_thres=195, C=100):\n",
    "    '''Train the agent'''\n",
    "\n",
    "    print(\"Training start:\")\n",
    "    \n",
    "    # counter of total steps taken\n",
    "    step_counter = 0   \n",
    "    \n",
    "    # record the return from every episode, for diagnose purpose\n",
    "    episode_return = []\n",
    "    \n",
    "    # early stop flag\n",
    "    early_stop = False\n",
    "    \n",
    "    for i in range(episodes):\n",
    "        state = agent.env.reset()\n",
    "        reward_rec = 0  # record reward of this episode\n",
    "        if early_stop:\n",
    "            break\n",
    "        while True:\n",
    "            # decay epsilon\n",
    "            if agent.memo.canStart():\n",
    "                # do not decay epsilon when the replay memory is not large enough, instead, use max_epsilon\n",
    "                epsilon = min_epsilon + (max_epsilon - min_epsilon) * math.exp(-1. * step_counter / eps_decay)\n",
    "            else:\n",
    "                epsilon = max_epsilon\n",
    "                \n",
    "            action = agent.chooseAction(state, epsilon)\n",
    "            new_state, reward, done, _ = agent.env.step(action)\n",
    "\n",
    "            step_counter += 1      # increment the counter of steps taken\n",
    "            reward_rec += reward   # record the reward of this transition\n",
    "            \n",
    "            # new_state = agent.__wrapState(new_state)\n",
    "            agent.memo.add(Experience(state, action, reward, new_state, done))\n",
    "            \n",
    "            # don't forget this assignment!!!!!\n",
    "            state = new_state\n",
    "            \n",
    "            if done:\n",
    "                episode_return.append(reward_rec)\n",
    "                if (i+1) % 100 == 0:\n",
    "                    # print something useful every 100 episodes\n",
    "                    mean_reward = np.mean(episode_return[-100:]).item()                  # mean reward of the last 100 episodes\n",
    "                    num_solve = np.sum(np.array(episode_return[-100:]) >= reward_thres)  # count of last 100 episodes that solved\n",
    "        \n",
    "                    # early stop if the current policy already solved problem and the exploration rate is low\n",
    "                    if mean_reward >= reward_thres and num_solve >= 90 and epsilon < min_epsilon + 0.05:\n",
    "                        early_stop = True\n",
    "\n",
    "                    print(\"  -Episode: {0}/{1};\\tTotal steps: {2};\\tMean rewards of last 100 runs: {3};\\t\\\n",
    "                    Count of solved episodes in last 100 runs: {4}\"\\\n",
    "                            .format(i+1, episodes, step_counter, mean_reward, num_solve))\n",
    "                break\n",
    "            \n",
    "            \n",
    "            # check if replay memory is big enough to start learning, learn every update_rate steps\n",
    "            if not agent.memo.canStart() or step_counter % update_rate != 0:\n",
    "                continue\n",
    "            \n",
    "            # =====================\n",
    "            # sample from experience memory and start learning\n",
    "            # =====================\n",
    "            batch = agent.memo.sample(batch_size=batch_size)\n",
    "            if batch is None:\n",
    "                continue\n",
    "            else:\n",
    "                # transform batch into predicted and true Q values\n",
    "                X, X_next, actions, rewards, not_dones = processBatch(batch)\n",
    "                \n",
    "                yPred = agent.pn(X).gather(1, actions.view(-1, 1))\n",
    "                yTrue = torch.zeros((len(batch), 1), dtype=torch.float32, device=device)\n",
    "                yTrue[not_dones, :] = agent.tn(X_next[not_dones, :]).detach().max(1)[0].view(-1, 1)\n",
    "                yTrue = rewards + yTrue * gamma\n",
    "    \n",
    "                loss = loss_fn(yPred, yTrue)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                \n",
    "#                 for param in agent.pn.parameters():\n",
    "#                     param.grad.data.clamp_(-1, 1)\n",
    "                    \n",
    "                optimizer.step()\n",
    "            \n",
    "            if step_counter % C == 0:   # update target network after C steps\n",
    "                agent.tn.load_state_dict(agent.pn.state_dict())\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "    plot_training(episode_return)\n",
    "    return episode_return\n",
    "\n",
    "\n",
    "def plot_training(episode_rewards, solve_thres=200):\n",
    "\n",
    "    is_ipython = 'inline' in matplotlib.get_backend()\n",
    "    if is_ipython:\n",
    "        from IPython import display\n",
    "    \n",
    "    x_ticks = len(episode_rewards)\n",
    "    y_max = max(episode_rewards)\n",
    "    y_min = min(episode_rewards)\n",
    "\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    plt.grid(True)\n",
    "    plt.plot(range(1, x_ticks+1), episode_rewards, c='b', label=\"episode returns\")\n",
    "    plt.plot(range(1, x_ticks+1), [solve_thres] * x_ticks, c='r', label=\"solving threshold\")\n",
    "    plt.title(\"Episode Return during Training\")\n",
    "    plt.legend()\n",
    "    # plt.savefig(\"illustrations/training.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "def plot_test(agent, solve_thres):\n",
    "    rewards = []\n",
    "    for i in range(100):\n",
    "        rewards.append(agent.play())\n",
    "    \n",
    "    num_solved = sum(np.array(rewards) >= solve_thres)\n",
    "\n",
    "    plt.figure(figsize=(3.54, 2.8))\n",
    "    plt.grid(True)\n",
    "    plt.scatter(range(1, 101), rewards, c='b', s=1, label=\"episode returns\")\n",
    "    plt.plot(range(1, 101), [solve_thres] * 100, c='r', label=\"Solving threshold\")\n",
    "\n",
    "    plt.title(\"Episode Return of 100 runs with greedy policy, solved {}/100\".format(num_solved))\n",
    "    plt.legend()\n",
    "    # plt.savefig(\"illustrations/test.png\", dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9jNVwVVA3vEA"
   },
   "source": [
    "## Start Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qI0MUCxz3uf9"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "# -----------------------\n",
    "NUM_EPISODES = 3000\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "GAMMA = 0.99\n",
    "MAX_EPSILON = 1\n",
    "MIN_EPSILON = 0.05\n",
    "EPS_DECAY = 50000\n",
    "EXPERIENCE_CAPACITY = 500000\n",
    "EXP_START_SIZE = 20000\n",
    "UPDATE_RATE = 1\n",
    "REWARD_THRES = 200\n",
    "C = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7542261,
     "status": "error",
     "timestamp": 1572316605860,
     "user": {
      "displayName": "Zhiqi Yu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBi-8Tmka31UxkN8y4ziOk_1QXx0krmLXgBn1kSng=s64",
      "userId": "01942841542704611946"
     },
     "user_tz": 240
    },
    "id": "IeZTy_69PXzR",
    "outputId": "4e6c7446-647e-4da6-f12d-0cde53811b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start:\n",
      "  -Episode: 100/3000;\tTotal steps: 8783;\tMean rewards of last 100 runs: -175.4990439516482;\t                    Count of solved episodes in last 100 runs: 0\n",
      "  -Episode: 200/3000;\tTotal steps: 17829;\tMean rewards of last 100 runs: -187.3332600734228;\t                    Count of solved episodes in last 100 runs: 0\n",
      "  -Episode: 300/3000;\tTotal steps: 33400;\tMean rewards of last 100 runs: -151.51886581977266;\t                    Count of solved episodes in last 100 runs: 0\n",
      "  -Episode: 400/3000;\tTotal steps: 69885;\tMean rewards of last 100 runs: -52.62258652746387;\t                    Count of solved episodes in last 100 runs: 0\n",
      "  -Episode: 500/3000;\tTotal steps: 152415;\tMean rewards of last 100 runs: -54.07681688465758;\t                    Count of solved episodes in last 100 runs: 5\n",
      "  -Episode: 600/3000;\tTotal steps: 241147;\tMean rewards of last 100 runs: -100.72976795433708;\t                    Count of solved episodes in last 100 runs: 0\n",
      "  -Episode: 700/3000;\tTotal steps: 320712;\tMean rewards of last 100 runs: -80.95581871631595;\t                    Count of solved episodes in last 100 runs: 1\n",
      "  -Episode: 800/3000;\tTotal steps: 386513;\tMean rewards of last 100 runs: -17.81150398863295;\t                    Count of solved episodes in last 100 runs: 6\n",
      "  -Episode: 900/3000;\tTotal steps: 417163;\tMean rewards of last 100 runs: 26.42585904356986;\t                    Count of solved episodes in last 100 runs: 19\n",
      "  -Episode: 1000/3000;\tTotal steps: 440941;\tMean rewards of last 100 runs: -7.708201848082272;\t                    Count of solved episodes in last 100 runs: 6\n",
      "  -Episode: 1100/3000;\tTotal steps: 463589;\tMean rewards of last 100 runs: 4.304327680940681;\t                    Count of solved episodes in last 100 runs: 9\n",
      "  -Episode: 1200/3000;\tTotal steps: 490261;\tMean rewards of last 100 runs: 21.068217288727563;\t                    Count of solved episodes in last 100 runs: 13\n",
      "  -Episode: 1300/3000;\tTotal steps: 529893;\tMean rewards of last 100 runs: 64.64621410448618;\t                    Count of solved episodes in last 100 runs: 25\n",
      "  -Episode: 1400/3000;\tTotal steps: 568709;\tMean rewards of last 100 runs: 128.16433946009482;\t                    Count of solved episodes in last 100 runs: 50\n",
      "  -Episode: 1500/3000;\tTotal steps: 613553;\tMean rewards of last 100 runs: 130.45568797492146;\t                    Count of solved episodes in last 100 runs: 53\n",
      "  -Episode: 1600/3000;\tTotal steps: 651011;\tMean rewards of last 100 runs: 178.09029558496084;\t                    Count of solved episodes in last 100 runs: 66\n",
      "  -Episode: 1700/3000;\tTotal steps: 692646;\tMean rewards of last 100 runs: 210.11028449953494;\t                    Count of solved episodes in last 100 runs: 79\n",
      "  -Episode: 1800/3000;\tTotal steps: 739346;\tMean rewards of last 100 runs: 190.3962961924135;\t                    Count of solved episodes in last 100 runs: 74\n",
      "  -Episode: 1900/3000;\tTotal steps: 805447;\tMean rewards of last 100 runs: 123.91114931895157;\t                    Count of solved episodes in last 100 runs: 49\n",
      "  -Episode: 2000/3000;\tTotal steps: 885324;\tMean rewards of last 100 runs: 69.22679874409509;\t                    Count of solved episodes in last 100 runs: 18\n",
      "  -Episode: 2100/3000;\tTotal steps: 954699;\tMean rewards of last 100 runs: 120.97833566198089;\t                    Count of solved episodes in last 100 runs: 33\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-198e2100029b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m reward_records = train(agent, optimizer, episodes=NUM_EPISODES, batch_size=BATCH_SIZE, update_rate=UPDATE_RATE, \n\u001b[0;32m---> 24\u001b[0;31m             gamma=GAMMA, max_epsilon=MAX_EPSILON, min_epsilon=MIN_EPSILON, eps_decay=EPS_DECAY, reward_thres=REWARD_THRES, C=C)\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# save training info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-7a7a422fddf0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(agent, optimizer, episodes, batch_size, update_rate, gamma, max_epsilon, min_epsilon, eps_decay, reward_thres, C)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;31m#                 for param in agent.pn.parameters():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "# environment attributes \n",
    "state_dim = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# The function approximation for Q function\n",
    "policy_net = QNN(state_dim, n_actions, [48])\n",
    "target_net = QNN(state_dim, n_actions, [48])\n",
    "\n",
    "# Experience replay\n",
    "exp_play = ExpReplay(EXPERIENCE_CAPACITY, EXP_START_SIZE)\n",
    "\n",
    "# make the learning agent\n",
    "agent = LunarLanderAgent(env, policy_net, target_net, exp_play)\n",
    "\n",
    "# set-up optimizer\n",
    "optimizer = Adam(agent.pn.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# start training\n",
    "reward_records = train(agent, optimizer, episodes=NUM_EPISODES, batch_size=BATCH_SIZE, update_rate=UPDATE_RATE, \n",
    "            gamma=GAMMA, max_epsilon=MAX_EPSILON, min_epsilon=MIN_EPSILON, eps_decay=EPS_DECAY, reward_thres=REWARD_THRES, C=C)\n",
    "\n",
    "# save training info\n",
    "# torch.save(agent.pn.state_dict(), \"checkpoint.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x37ih96GBh0S",
    "outputId": "5868348c-a202-4cd1-f486-4e40818c7bae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 221\n"
     ]
    }
   ],
   "source": [
    "agent.play(render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2aHCbmXFPXzY",
    "outputId": "d01a4125-c4b2-4b82-fe2a-fc3310a2ee8f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204.6834266441645"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards = 0\n",
    "for i in range(100):\n",
    "    rewards += agent.play()\n",
    "\n",
    "rewards/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ELaH7tQNOPJl",
    "outputId": "16dbd60b-ec4b-40e0-899e-128377e96b67"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_test(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hnZ0O30cX75P"
   },
   "source": [
    "## Trained Agent Performance\n",
    "\n",
    "![trial](static/ll.gif)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ll-dqn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
